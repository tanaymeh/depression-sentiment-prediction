{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wlu35AFdCBHh"
   },
   "source": [
    "# Depression Sentiment Prediction (Part-2)\n",
    "## 5. Importing Libraries and Feature Extracted Data\n",
    "### In the last notebook, we have successfully cleaned, organised a feature extracted the Data. Now is the time to import it and proceed with further operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7wLq54QxAIlB"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "try:\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.naive_bayes import MultinomialNB\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from xgboost import XGBClassifier\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem import PorterStemmer\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from wordcloud import WordCloud\n",
    "except:\n",
    "    print(\"Required Libraries not found, Installing them...\\nOnce done, please re-run the notebook\")\n",
    "    os.system(\"pip install numpy pandas sklearn scipy xgboost nltk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "id": "QdB-GPDAEPBW",
    "outputId": "2fffa676-3393-4080-e5c5-dc33e0c33da8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>upset cant updat facebook text might cri resul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>kenichan dive mani time ball manag save rest b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>whole bodi feel itchi like fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>nationwideclass behav mad cant see</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>kwesidei whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                               text\n",
       "0       1  upset cant updat facebook text might cri resul...\n",
       "1       1  kenichan dive mani time ball manag save rest b...\n",
       "2       1                   whole bodi feel itchi like fire \n",
       "3       1                nationwideclass behav mad cant see \n",
       "4       1                               kwesidei whole crew "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the same encoding 'latin-1' as used in the previous notebook\n",
    "data = pd.read_csv('feature_extracted.csv', encoding='latin-1')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7NEAHTrPHrK-"
   },
   "outputs": [],
   "source": [
    "# Also, let's just drop the NaN values. Don't know how I forgot that in the last notebook!\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wjdQLYxyJ2Az"
   },
   "source": [
    "## 6. Splitting the Dataset\n",
    "### Before we vectorize our data, let's first split it into Training and Test Set as it will be complicated to do so after Vectorizing it with TfIdf Vectorizer.\n",
    "### We will split the data so that there is 98% (about *1,279,489* data points) in Training Set and remaning 2% (about *319,873* data points) in the Test Set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PPdcAPuYJhVo",
    "outputId": "930f859a-b62e-40f3-9e55-b9f37534f690"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Size: 1567374 and Testing Data Size: 31988\n"
     ]
    }
   ],
   "source": [
    "text = data['text']\n",
    "target = data['target']\n",
    "trainX, testX, trainY, testY = train_test_split(text, target, test_size=0.02)\n",
    "\n",
    "print(\"Training Data Size: {} and Testing Data Size: {}\".format(trainX.shape[0], testX.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bu97IfYoNJUo"
   },
   "source": [
    "## 7. Vectorizing data\n",
    "### Machine Learning Models can't directly work on just text data (which is basically a collection of *strings*), so we have to find a way to convert this sequential data to normal numerical data that can be understood by these algorithms.\n",
    "### One such widely used method is Tf-Idf Vectorizing. It's Basically replacing every word in the dataset with the number of times it appears in the Dataset. So that's just a word frequency counter and replacer.\n",
    "#### Note: I know I could have made myself a Tf-idf Vectorizer from scratch (and I even tried that as an experiment, it worked!)but the problem with that solution is that it's very resource-inefficient and time consuming since our data is just humongous in size and word diversity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s3tbAr-sLcfD"
   },
   "outputs": [],
   "source": [
    "# Let's Initialize our Vectorizer\n",
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "JOpdzylzPpm1",
    "outputId": "94f1db0c-1749-4fab-e33e-165ad627bc64"
   },
   "outputs": [],
   "source": [
    "# Now let's Vectorize our Training Text data and also time it (just for fun!)\n",
    "train_features = vectorizer.fit_transform(trainX)\n",
    "# Also, let's just convert our training target values (trainY) into a numpy array\n",
    "train_targets = np.array(trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "XG3sLkGiT0We",
    "outputId": "154346c6-989a-48ab-ce5c-7ed333a28559"
   },
   "outputs": [],
   "source": [
    "# Also, let's convert our Test Data into similar form for future ease of use\n",
    "test_features = vectorizer.transform(testX)\n",
    "test_targets = np.array(testY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UISib_gxSP_k"
   },
   "source": [
    "## 8. Training Machine Learning Models\n",
    "### Now comes the fun part! Let's just start training different model on our data and see which one performs best in-terms of ROC_AUC Value, Confusion Metrics and Obviously, how much it takes to train! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-AIv73IQTLkw"
   },
   "source": [
    "### 8.1 Logistic Regression\n",
    "#### Findings: Our Model performs Substantially OK on the Test Set, however the number of False Positives and False Negatives aren't very low, so we will try other models too.\n",
    "##### ROC-AUC Score: 0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "sJnSyRfXRFnE",
    "outputId": "1d7233f3-2fd4-47af-8ee0-fb69c6feb75e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/heytanay/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's first try the Simple, Good-old Logistic Regression\n",
    "lr_classifier = LogisticRegression(C=1.)\n",
    "lr_classifier.fit(train_features, train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "colab_type": "code",
    "id": "v12S7Hw2SUK_",
    "outputId": "0eedcffb-ea6b-4e07-fbca-8dc6949e4304"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC Value is: 0.8506090412770613\n",
      "\n",
      "Total Size of Test Data: 31988\n",
      "True Positives are: 12526\n",
      "True Negatives are: 12139\n",
      "False Positives are: 3481\n",
      "False Negatives are: 3842\n"
     ]
    }
   ],
   "source": [
    "# Let's Test the Model on our Test Data\n",
    "predictions = lr_classifier.predict_proba(test_features)\n",
    "\n",
    "# Let us get the ROC-AUC Score and Confusion Metrics\n",
    "roc_auc_lr = roc_auc_score(test_targets, predictions[:,-1])\n",
    "confusion_lr = confusion_matrix(test_targets, np.round(predictions[:,-1]))\n",
    "\n",
    "# Assign the True Positives (lr_tp), True Negatives (lr_tn), False Positives (lr_fp) and False Negatives (lr_fn)\n",
    "lr_tp = confusion_lr[0][0]\n",
    "lr_tn = confusion_lr[1][1]\n",
    "lr_fp = confusion_lr[0][1]\n",
    "lr_fn = confusion_lr[1][0]\n",
    "\n",
    "# Print the Results\n",
    "print(\"ROC-AUC Value is: {}\".format(roc_auc_lr))\n",
    "print(\"\\nTotal Size of Test Data: {}\".format(testX.shape[0]))\n",
    "print(\"True Positives are: {}\\nTrue Negatives are: {}\\nFalse Positives are: {}\\nFalse Negatives are: {}\".format(lr_tp, lr_tn, lr_fp, lr_fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AGuRD9GBEY9R"
   },
   "source": [
    "### 8.2 Naive Bayes\n",
    "#### Findings: Multinomial Naive Bayes performs slightly Less Better than our Previous Logistic Regression model, but still it's performance on Test Model is substantial and well above the Benchmark I set in the Proposal (0.7 on Test Set). However comparing the training time of both models, the time v/s performance tradeoff is obvious.\n",
    "\n",
    "##### ROC-AUC Score: 0.83"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "s56Ec64jBq0h",
    "outputId": "b155a4b4-f4d2-4095-b7af-4bc5f519dd7e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try Multinomial Naive Bayes on our data\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(train_features, train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "colab_type": "code",
    "id": "UoNzDIDLFDil",
    "outputId": "23827da7-40b1-4f43-bab9-3bce23158b6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC Value is: 0.832812297754705\n",
      "\n",
      "Total Size of Test Data: 31988\n",
      "True Positives are: 10944\n",
      "True Negatives are: 12890\n",
      "False Positives are: 5063\n",
      "False Negatives are: 3091\n"
     ]
    }
   ],
   "source": [
    "# Let's test the Naive Bayes Classifer on our Test Data\n",
    "predictions = nb_classifier.predict_proba(test_features)\n",
    "\n",
    "# Let us get the ROC-AUC Score and Confusion Metrics\n",
    "roc_auc_nb = roc_auc_score(test_targets, predictions[:,-1])\n",
    "confusion_nb = confusion_matrix(test_targets, np.round(predictions[:,-1]))\n",
    "\n",
    "# Assign the True Positives (lr_tp), True Negatives (lr_tn), False Positives (lr_fp) and False Negatives (lr_fn)\n",
    "nb_tp = confusion_nb[0][0]\n",
    "nb_tn = confusion_nb[1][1]\n",
    "nb_fp = confusion_nb[0][1]\n",
    "nb_fn = confusion_nb[1][0]\n",
    "\n",
    "# Print the Results\n",
    "print(\"ROC-AUC Value is: {}\".format(roc_auc_nb))\n",
    "print(\"\\nTotal Size of Test Data: {}\".format(testX.shape[0]))\n",
    "print(\"True Positives are: {}\\nTrue Negatives are: {}\\nFalse Positives are: {}\\nFalse Negatives are: {}\".format(nb_tp, nb_tn, nb_fp, nb_fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Random Forest Classifier\n",
    "#### Findings: Quite Suprisingly, Random Forest Classifier has worked out to be lesser Efficient than I previously thought. But, all these models have still managed to get better results on test-set than the ones I set for Benchmarks (0.7 on test-set). So I am hitting the benchmark pretty resonably.\n",
    "#### ROC-AUC Score: 0.80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/heytanay/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try the model with random forest classifier\n",
    "rf_classifier = RandomForestClassifier()\n",
    "rf_classifier.fit(train_features, train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC Value is: 0.8065653274064477\n",
      "\n",
      "Total Size of Test Data: 31988\n",
      "True Positives are: 12460\n",
      "True Negatives are: 11084\n",
      "False Positives are: 3547\n",
      "False Negatives are: 4897\n"
     ]
    }
   ],
   "source": [
    "# Let's test the Naive Bayes Classifer on our Test Data\n",
    "predictions = rf_classifier.predict_proba(test_features)\n",
    "\n",
    "# Let us get the ROC-AUC Score and Confusion Metrics\n",
    "roc_auc_rf = roc_auc_score(test_targets, predictions[:,-1])\n",
    "confusion_rf = confusion_matrix(test_targets, np.round(predictions[:,-1]))\n",
    "\n",
    "# Assign the True Positives (lr_tp), True Negatives (lr_tn), False Positives (lr_fp) and False Negatives (lr_fn)\n",
    "rf_tp = confusion_rf[0][0]\n",
    "rf_tn = confusion_rf[1][1]\n",
    "rf_fp = confusion_rf[0][1]\n",
    "rf_fn = confusion_rf[1][0]\n",
    "\n",
    "# Print the Results\n",
    "print(\"ROC-AUC Value is: {}\".format(roc_auc_rf))\n",
    "print(\"\\nTotal Size of Test Data: {}\".format(testX.shape[0]))\n",
    "print(\"True Positives are: {}\\nTrue Negatives are: {}\\nFalse Positives are: {}\\nFalse Negatives are: {}\".format(rf_tp, rf_tn, rf_fp, rf_fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 Decision Tree Classifier\n",
    "#### Findings: Decision Tree Classifier actually works equal to the benchmark I set (0.7 on test set), Which makes it not a very smart choice to use.\n",
    "#### ROC-AUC Score: 0.70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let us try Decision Tree Classifier on our data\n",
    "dt_classifier = DecisionTreeClassifier()\n",
    "dt_classifier.fit(train_features, train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC Value is: 0.7042962306550173\n",
      "\n",
      "Total Size of Test Data: 31988\n",
      "True Positives are: 11135\n",
      "True Negatives are: 11396\n",
      "False Positives are: 4872\n",
      "False Negatives are: 4585\n"
     ]
    }
   ],
   "source": [
    "# Let's test the Naive Bayes Classifer on our Test Data\n",
    "predictions = dt_classifier.predict_proba(test_features)\n",
    "\n",
    "# Let us get the ROC-AUC Score and Confusion Metrics\n",
    "roc_auc_dt = roc_auc_score(test_targets, predictions[:,-1])\n",
    "confusion_dt = confusion_matrix(test_targets, np.round(predictions[:,-1]))\n",
    "\n",
    "# Assign the True Positives (lr_tp), True Negatives (lr_tn), False Positives (lr_fp) and False Negatives (lr_fn)\n",
    "dt_tp = confusion_dt[0][0]\n",
    "dt_tn = confusion_dt[1][1]\n",
    "dt_fp = confusion_dt[0][1]\n",
    "dt_fn = confusion_dt[1][0]\n",
    "\n",
    "# Print the Results\n",
    "print(\"ROC-AUC Value is: {}\".format(roc_auc_dt))\n",
    "print(\"\\nTotal Size of Test Data: {}\".format(testX.shape[0]))\n",
    "print(\"True Positives are: {}\\nTrue Negatives are: {}\\nFalse Positives are: {}\\nFalse Negatives are: {}\".format(dt_tp, dt_tn, dt_fp, dt_fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5 K-Nearest Neighbors Classifier\n",
    "#### Findings: KNN Classifier did an 'OK' job our test data. Though, not as good as Logistic Regression or Naive Bayes, it still managed to pass the benchmark line (0.7)\n",
    "#### ROC-AUC Score: 0.72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let us also try KNN Classifier on our data\n",
    "kn_classifier = KNeighborsClassifier()\n",
    "kn_classifier.fit(train_features, train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC Value is: 0.7263250645063234\n",
      "\n",
      "Total Size of Test Data: 31988\n",
      "True Positives are: 10125\n",
      "True Negatives are: 11323\n",
      "False Positives are: 5882\n",
      "False Negatives are: 4658\n"
     ]
    }
   ],
   "source": [
    "# Let's test the Naive Bayes Classifer on our Test Data\n",
    "predictions = kn_classifier.predict_proba(test_features)\n",
    "\n",
    "# Let us get the ROC-AUC Score and Confusion Metrics\n",
    "roc_auc_kn = roc_auc_score(test_targets, predictions[:,-1])\n",
    "confusion_kn = confusion_matrix(test_targets, np.round(predictions[:,-1]))\n",
    "\n",
    "# Assign the True Positives (lr_tp), True Negatives (lr_tn), False Positives (lr_fp) and False Negatives (lr_fn)\n",
    "kn_tp = confusion_kn[0][0]\n",
    "kn_tn = confusion_kn[1][1]\n",
    "kn_fp = confusion_kn[0][1]\n",
    "kn_fn = confusion_kn[1][0]\n",
    "\n",
    "# Print the Results\n",
    "print(\"ROC-AUC Value is: {}\".format(roc_auc_kn))\n",
    "print(\"\\nTotal Size of Test Data: {}\".format(testX.shape[0]))\n",
    "print(\"True Positives are: {}\\nTrue Negatives are: {}\\nFalse Positives are: {}\\nFalse Negatives are: {}\".format(kn_tp, kn_tn, kn_fp, kn_fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.6 XGBOOST Classifier\n",
    "#### Findings: XGB Classifier worked substantially OK, although it crossed the Benchmark line having a score of 0.76, I still believe, this model can do better and so I will be using GridSearchCV on this model along with Logistic Regression and Naive Bayes\n",
    "#### ROC-AUC Score: 0.76"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
       "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
       "       n_estimators=100, n_jobs=1, nthread=None,\n",
       "       objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=None,\n",
       "       subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try the final XGBoost Classifier\n",
    "xg_classifier = XGBClassifier()\n",
    "xg_classifier.fit(train_features, train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC Value is: 0.762598276541667\n",
      "\n",
      "Total Size of Test Data: 31988\n",
      "True Positives are: 13733\n",
      "True Negatives are: 8431\n",
      "False Positives are: 2274\n",
      "False Negatives are: 7550\n"
     ]
    }
   ],
   "source": [
    "# Let's test the Naive Bayes Classifer on our Test Data\n",
    "predictions = xg_classifier.predict_proba(test_features)\n",
    "\n",
    "# Let us get the ROC-AUC Score and Confusion Metrics\n",
    "roc_auc_xg = roc_auc_score(test_targets, predictions[:,-1])\n",
    "confusion_xg = confusion_matrix(test_targets, np.round(predictions[:,-1]))\n",
    "\n",
    "# Assign the True Positives (lr_tp), True Negatives (lr_tn), False Positives (lr_fp) and False Negatives (lr_fn)\n",
    "xg_tp = confusion_xg[0][0]\n",
    "xg_tn = confusion_xg[1][1]\n",
    "xg_fp = confusion_xg[0][1]\n",
    "xg_fn = confusion_xg[1][0]\n",
    "\n",
    "# Print the Results\n",
    "print(\"ROC-AUC Value is: {}\".format(roc_auc_xg))\n",
    "print(\"\\nTotal Size of Test Data: {}\".format(testX.shape[0]))\n",
    "print(\"True Positives are: {}\\nTrue Negatives are: {}\\nFalse Positives are: {}\\nFalse Negatives are: {}\".format(xg_tp, xg_tn, xg_fp, xg_fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Optimal Hyperparamter Search using GridSearchCV\n",
    "### In this final section, We will search for Optimal Hyperparamters for 4 models, I found to be worth the time:\n",
    "#### 1. Logistic Regression\n",
    "#### 2. Naive Bayes\n",
    "#### 3. XGBoost Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 Logistic Regression Hyperparamter Search\n",
    "#### Best Paramter found: C=0.76\n",
    "Clearly there hasn't been much of an improvement in our Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/heytanay/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=None, param_grid={'C': [1, 10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let us define the parameter we would like to do search on\n",
    "parameters_lr = {'C':[1,10]}\n",
    "\n",
    "# Now, let's fit our GCV model to the data\n",
    "gcv_lr = GridSearchCV(lr_classifier, parameters_lr, cv=5)\n",
    "gcv_lr.fit(train_features, train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score found is: 0.770955\n",
      "New ROC-AUC Score is found to be: 0.8506090373678774\n"
     ]
    }
   ],
   "source": [
    "# Best Paramters are:\n",
    "print(\"Best Score found is: {}\".format(gcv_lr.best_score_))\n",
    "\n",
    "# Let us recalculate and see if their is any change in our ROC-AUC Score\n",
    "probs = gcv_lr.predict_proba(test_features)\n",
    "roc_auc_gcv_lr = roc_auc_score(test_targets, probs[:,-1])\n",
    "print(\"New ROC-AUC Score is found to be: {}\".format(roc_auc_gcv_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Naive Bayes Hyperparameter Search\n",
    "#### Best Parameter found: C=0.73\n",
    "So, far there is no change in Naive Bayes too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/heytanay/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv='warn', error_score='raise-deprecating',\n",
       "       estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'alpha': (1, 0.1, 0.01, 0.001, 0.0001, 1e-05)},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's define parameters (Only, alpha)\n",
    "parameters_nb = {  \n",
    "    'alpha': (1, 0.1, 0.01, 0.001, 0.0001, 0.00001)  \n",
    "}\n",
    "\n",
    "gcv_nb = GridSearchCV(nb_classifier, parameters_nb)\n",
    "gcv_nb.fit(train_features, train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score found is: 0.74708\n",
      "New ROC-AUC Score is found to be: 0.832812297754705\n"
     ]
    }
   ],
   "source": [
    "# Best Paramters are:\n",
    "print(\"Best Score found is: {}\".format(gcv_nb.best_score_))\n",
    "\n",
    "# Let us recalculate and see if their is any change in our ROC-AUC Score\n",
    "probs = gcv_nb.predict_proba(test_features)\n",
    "roc_auc_gcv_nb = roc_auc_score(test_targets, probs[:,-1])\n",
    "print(\"New ROC-AUC Score is found to be: {}\".format(roc_auc_gcv_nb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 XGBoost Classifier Hyperparameter Search\n",
    "#### Best Parameter found: C="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/heytanay/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv='warn', error_score='raise-deprecating',\n",
       "       estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
       "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
       "       n_estimators=100, n_jobs=1, nthread=None,\n",
       "       objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=None,\n",
       "       subsample=1, verbosity=1),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'alpha': [0.0001, 0.001, 0.01, 0.1, 1, 10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters_xgb = {'alpha':[0.0001, 0.001, 0.01, 0.1, 1, 10]}\n",
    "gcv_xgb = GridSearchCV(xg_classifier, parameters_xgb)\n",
    "gcv_xgb.fit(train_features, train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score found is: 0.688375\n",
      "New ROC-AUC Score is found to be: 0.762598276541667\n"
     ]
    }
   ],
   "source": [
    "# Best Paramters are:\n",
    "print(\"Best Score found is: {}\".format(gcv_xgb.best_score_))\n",
    "\n",
    "# Let us recalculate and see if their is any change in our ROC-AUC Score\n",
    "probs = gcv_xgb.predict_proba(test_features)\n",
    "roc_auc_gcv_xgb = roc_auc_score(test_targets, probs[:,-1])\n",
    "print(\"New ROC-AUC Score is found to be: {}\".format(roc_auc_gcv_xgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing on real world Sentences\n",
    "Let's now test our best performing model `lr_classifier` on both positive and negative real world sentence.\n",
    "For this, we need to copy the `process_text()` function from the first notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    \"\"\"\n",
    "    @param: text (Raw Text sentence)\n",
    "    @return: final_str (Final processed string)\n",
    "    \n",
    "    Function -> It takes raw string as an Input and processed data to get important features extracted\n",
    "                First it converts to lower, then applies regex code, then tokenizes the words, then removes\n",
    "                stopwords, then stems the words, then puts the output list back into a string.\n",
    "    \"\"\"\n",
    "    import re\n",
    "    \n",
    "    # Convert text to lower and remove all special characters from it using regex\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^(a-zA-Z)\\s]','', text)\n",
    "    \n",
    "    # Tokenize the words using the word_tokenize() from nltk lib\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Only take the words whose length is greater than 2\n",
    "    words = [w for w in words if len(w) > 2]\n",
    "    \n",
    "    # Get the stopwords for english language\n",
    "    sw = stopwords.words('english')\n",
    "    \n",
    "    # Get only those words which are not in stopwords (those which are not stopwords)\n",
    "    words = [word for word in words if word not in sw]\n",
    "    \n",
    "    # Get the PorterStemmer algorithm module\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    # Take the words with commoner morphological and inflexional endings from words removed\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    \n",
    "    # Till this point, we have a list of strings (words), we want them to be converted to a string of text\n",
    "    final_str = \"\"\n",
    "    for w in words:\n",
    "        final_str += w\n",
    "        final_str += \" \"\n",
    "    \n",
    "    # Return the final string\n",
    "    return final_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positive Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chances of Text Being Positive: 79.16412778833273 %\n",
      "Is the Person Depressed? False\n"
     ]
    }
   ],
   "source": [
    "# Make a sentence and process it\n",
    "positive_sentence = \"Having a really good time here!\"\n",
    "pos_sent = process_text(positive_sentence)\n",
    "\n",
    "# Transform the sentence using TfIdf Vectorizer\n",
    "vect_pos = vectorizer.transform(np.array([pos_sent]))\n",
    "\n",
    "# Predict the Probabilities of the Sentence being Positive and Negative\n",
    "pred_pos = lr_classifier.predict_proba(vect_pos)\n",
    "\n",
    "# The First value in the prediction array is the %-chances of the text being positive and the second value is it\n",
    "# being negative\n",
    "isDepressed = pred_pos[0][0] < 0.3\n",
    "print(\"Chances of Text Being Positive: {} %\".format(pred_pos[0][0]*100))\n",
    "print(\"Is the Person Depressed? {}\".format(isDepressed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Depressive Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chances of Text Being Positive: 0.01424067000929119 %\n",
      "Is the Person Depressed? True\n"
     ]
    }
   ],
   "source": [
    "# Make a sentence and process it\n",
    "negative_sentence = \"Really sad that he left us and is never coming back. feel like crying\"\n",
    "neg_sent = process_text(negative_sentence)\n",
    "\n",
    "# Transform the sentence using TfIdf Vectorizer\n",
    "vect_neg = vectorizer.transform(np.array([neg_sent]))\n",
    "\n",
    "# Predict the Probabilities of the Sentence being Positive and Negative\n",
    "pred_neg = lr_classifier.predict_proba(vect_neg)\n",
    "\n",
    "# The First value in the prediction array is the %-chances of the text being positive and the second value is it\n",
    "# being negative\n",
    "isDepressed = pred_neg[0][0] < 0.3\n",
    "print(\"Chances of Text Being Positive: {} %\".format(pred_neg[0][0]*100))\n",
    "print(\"Is the Person Depressed? {}\".format(isDepressed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "Finally, I would like to Conclude this Project with my best Result being the Logistic Regression Model for the Classification Purposes. Alternatively, I have provided all the different models used in the project for the user and reviewers to use and play around with. Models are in the ```models``` folders in the Github Repository of this project.\n",
    "\n",
    "\n",
    "This is Tanay Mehta, Signing Out!"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": "c4dd7ddf9fbc45e38330719afd1e50f2",
   "lastKernelId": "2e338e9b-0a4d-401d-9075-7648f056b3c2"
  },
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Depression Model Part-2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
